{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1 DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP6JLo1tGNBg"
      },
      "source": [
        "# ANN\n",
        "#Predict the electrical energy output for the\n",
        "#Combined Cycle Power Plant Data Set\n",
        "\n",
        "\n",
        "*Adhiraj Roka*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWZyYmS_UE_L"
      },
      "source": [
        "### Importing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxkJoQBkUIHC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "!pip install -q xlrd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKWAkFVGUU0Z"
      },
      "source": [
        "### Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhiSahyYXH5_"
      },
      "source": [
        "Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDy1cOS6Z1r4",
        "outputId": "33603121-24c5-478d-c350-03737fb49493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyqVl2BDXdV6"
      },
      "source": [
        "Setting up directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0MgnKUmaKAx",
        "outputId": "a2c729b8-cd72-40e3-8b47-73522b1215e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg76uCRZXg0V"
      },
      "source": [
        "Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXUkhkMfU4wq"
      },
      "source": [
        "energy = pd.read_excel('Folds5x2_pp.xlsx')\n",
        "X=energy.iloc[:,0:4].values\n",
        "Y=energy.iloc[:,-1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYP9cQTWbzuI",
        "outputId": "135ab5c5-e9f8-4727-94d7-c0890d547454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "print(energy)\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         AT      V       AP     RH      PE\n",
            "0     14.96  41.76  1024.07  73.17  463.26\n",
            "1     25.18  62.96  1020.04  59.08  444.37\n",
            "2      5.11  39.40  1012.16  92.14  488.56\n",
            "3     20.86  57.32  1010.24  76.64  446.48\n",
            "4     10.82  37.50  1009.23  96.62  473.90\n",
            "...     ...    ...      ...    ...     ...\n",
            "9563  16.65  49.69  1014.01  91.00  460.03\n",
            "9564  13.19  39.18  1023.67  66.78  469.62\n",
            "9565  31.32  74.33  1012.92  36.48  429.57\n",
            "9566  24.48  69.45  1013.86  62.39  435.74\n",
            "9567  21.60  62.52  1017.23  67.87  453.28\n",
            "\n",
            "[9568 rows x 5 columns]\n",
            "[[  14.96   41.76 1024.07   73.17]\n",
            " [  25.18   62.96 1020.04   59.08]\n",
            " [   5.11   39.4  1012.16   92.14]\n",
            " ...\n",
            " [  31.32   74.33 1012.92   36.48]\n",
            " [  24.48   69.45 1013.86   62.39]\n",
            " [  21.6    62.52 1017.23   67.87]]\n",
            "[463.26 444.37 488.56 ... 429.57 435.74 453.28]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHol938cW8zd"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-TDt0Y_XEfc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, \n",
        "                                                    random_state = 0) \n",
        "#Target is continuous so no need to stratify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE_FcHyfV3TQ"
      },
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW_wgSCbn1t5"
      },
      "source": [
        "#Apply standardization to all inputs#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViCrE00rV8Sk"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "#print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zfEzkRVXIwF"
      },
      "source": [
        "## Building the ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvdeScabXtlB"
      },
      "source": [
        "### Initializing the ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dtrScHxXQox"
      },
      "source": [
        "tf.random.set_seed(123)\n",
        "ann = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP6urV6SX7kS"
      },
      "source": [
        "### Adding the input layer and the first hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bppGycBXYCQr"
      },
      "source": [
        "ann.add(tf.keras.layers.Dense(units=3, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BELWAc_8YJze"
      },
      "source": [
        "### Adding the second hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JneR0u0sYRTd"
      },
      "source": [
        "ann.add(tf.keras.layers.Dense(units=3, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyNEe6RXYcU4"
      },
      "source": [
        "### Adding the output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn3x41RBYfvY"
      },
      "source": [
        "ann.add(tf.keras.layers.Dense(units=1, activation='relu')) \n",
        "#can also just use \"linear\" activation function. Relu gave better mse."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GWlJChhY_ZI"
      },
      "source": [
        "### Compiling the ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG3RrwDXZEaS"
      },
      "source": [
        "ann.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['mse','mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QR_G5u7ZLSM"
      },
      "source": [
        "### Training the ANN on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHZ-LKv_ZRb3",
        "outputId": "c79757dd-03e4-4e33-8b4b-013b24730cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4134
        }
      },
      "source": [
        "ann.fit(X_train, y_train, validation_data= (X_test, y_test),  batch_size = 32, \n",
        "        epochs = 120)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 206233.5000 - mse: 206233.5000 - mae: 453.7996 - val_loss: 205651.7656 - val_mse: 205651.7656 - val_mae: 453.1364\n",
            "Epoch 2/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 202539.6875 - mse: 202539.6875 - mae: 449.6234 - val_loss: 198312.2656 - val_mse: 198312.2656 - val_mae: 444.8155\n",
            "Epoch 3/120\n",
            "240/240 [==============================] - 0s 987us/step - loss: 189076.4375 - mse: 189076.4375 - mae: 434.0835 - val_loss: 178059.5000 - val_mse: 178059.5000 - val_mae: 421.0169\n",
            "Epoch 4/120\n",
            "240/240 [==============================] - 0s 993us/step - loss: 160652.8750 - mse: 160652.8750 - mae: 398.8146 - val_loss: 141092.8438 - val_mse: 141092.8438 - val_mae: 372.5876\n",
            "Epoch 5/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 118971.9453 - mse: 118971.9453 - mae: 338.3445 - val_loss: 97760.6797 - val_mse: 97760.6797 - val_mae: 303.1222\n",
            "Epoch 6/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 78701.1094 - mse: 78701.1094 - mae: 264.5561 - val_loss: 62061.3633 - val_mse: 62061.3633 - val_mae: 229.7147\n",
            "Epoch 7/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 50114.9883 - mse: 50114.9883 - mae: 203.2617 - val_loss: 39840.4102 - val_mse: 39840.4102 - val_mae: 179.2252\n",
            "Epoch 8/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 33507.8398 - mse: 33507.8398 - mae: 162.4390 - val_loss: 27404.3887 - val_mse: 27404.3887 - val_mae: 145.0819\n",
            "Epoch 9/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 24097.9824 - mse: 24097.9824 - mae: 134.4622 - val_loss: 20439.9297 - val_mse: 20439.9297 - val_mae: 123.2127\n",
            "Epoch 10/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 18624.3223 - mse: 18624.3223 - mae: 117.2786 - val_loss: 16357.1250 - val_mse: 16357.1250 - val_mae: 110.3548\n",
            "Epoch 11/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 15137.1396 - mse: 15137.1396 - mae: 105.9558 - val_loss: 13525.0029 - val_mse: 13525.0029 - val_mae: 100.8519\n",
            "Epoch 12/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 12453.7002 - mse: 12453.7002 - mae: 96.4134 - val_loss: 11086.8887 - val_mse: 11086.8887 - val_mae: 91.4648\n",
            "Epoch 13/120\n",
            "240/240 [==============================] - 0s 994us/step - loss: 10046.3945 - mse: 10046.3945 - mae: 86.4795 - val_loss: 8829.0127 - val_mse: 8829.0127 - val_mae: 81.2361\n",
            "Epoch 14/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 7831.3560 - mse: 7831.3560 - mae: 75.7444 - val_loss: 6757.5879 - val_mse: 6757.5879 - val_mae: 70.0614\n",
            "Epoch 15/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 5911.9790 - mse: 5911.9790 - mae: 64.7740 - val_loss: 5068.4985 - val_mse: 5068.4985 - val_mae: 59.5800\n",
            "Epoch 16/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 4443.1396 - mse: 4443.1396 - mae: 55.0610 - val_loss: 3833.2837 - val_mse: 3833.2837 - val_mae: 50.8229\n",
            "Epoch 17/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 3385.8401 - mse: 3385.8401 - mae: 47.2134 - val_loss: 2941.3013 - val_mse: 2941.3013 - val_mae: 43.8949\n",
            "Epoch 18/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 2617.1970 - mse: 2617.1970 - mae: 40.8635 - val_loss: 2277.0613 - val_mse: 2277.0613 - val_mae: 38.1656\n",
            "Epoch 19/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 2043.2505 - mse: 2043.2505 - mae: 35.7272 - val_loss: 1784.7419 - val_mse: 1784.7419 - val_mae: 33.5867\n",
            "Epoch 20/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 1609.5453 - mse: 1609.5453 - mae: 31.4588 - val_loss: 1402.1404 - val_mse: 1402.1404 - val_mae: 29.4514\n",
            "Epoch 21/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 1269.7408 - mse: 1269.7408 - mae: 27.6917 - val_loss: 1096.0814 - val_mse: 1096.0814 - val_mae: 25.6756\n",
            "Epoch 22/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 996.7081 - mse: 996.7081 - mae: 24.1662 - val_loss: 844.8826 - val_mse: 844.8826 - val_mae: 22.1638\n",
            "Epoch 23/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 767.3194 - mse: 767.3194 - mae: 20.7759 - val_loss: 638.8833 - val_mse: 638.8833 - val_mae: 19.2041\n",
            "Epoch 24/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 574.5448 - mse: 574.5448 - mae: 17.7024 - val_loss: 464.7776 - val_mse: 464.7776 - val_mae: 15.7974\n",
            "Epoch 25/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 417.1778 - mse: 417.1778 - mae: 14.5707 - val_loss: 328.2589 - val_mse: 328.2589 - val_mae: 12.9184\n",
            "Epoch 26/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 290.1673 - mse: 290.1673 - mae: 11.7154 - val_loss: 222.1443 - val_mse: 222.1443 - val_mae: 10.4991\n",
            "Epoch 27/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 194.0173 - mse: 194.0173 - mae: 9.3200 - val_loss: 144.9268 - val_mse: 144.9268 - val_mae: 7.9272\n",
            "Epoch 28/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 128.1139 - mse: 128.1139 - mae: 7.3225 - val_loss: 95.4753 - val_mse: 95.4753 - val_mae: 6.4687\n",
            "Epoch 29/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 85.7320 - mse: 85.7320 - mae: 5.9483 - val_loss: 63.5834 - val_mse: 63.5834 - val_mae: 5.0219\n",
            "Epoch 30/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 59.5133 - mse: 59.5133 - mae: 4.9557 - val_loss: 45.7821 - val_mse: 45.7821 - val_mae: 4.4431\n",
            "Epoch 31/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 44.0892 - mse: 44.0892 - mae: 4.4085 - val_loss: 35.6175 - val_mse: 35.6175 - val_mae: 4.0852\n",
            "Epoch 32/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 35.0044 - mse: 35.0044 - mae: 4.0765 - val_loss: 29.6062 - val_mse: 29.6062 - val_mae: 3.8474\n",
            "Epoch 33/120\n",
            "240/240 [==============================] - 0s 998us/step - loss: 29.7353 - mse: 29.7353 - mae: 3.8936 - val_loss: 25.8473 - val_mse: 25.8473 - val_mae: 3.7180\n",
            "Epoch 34/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 26.5900 - mse: 26.5900 - mae: 3.7852 - val_loss: 23.8641 - val_mse: 23.8641 - val_mae: 3.6532\n",
            "Epoch 35/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 24.6588 - mse: 24.6588 - mae: 3.7236 - val_loss: 21.9466 - val_mse: 21.9466 - val_mae: 3.5857\n",
            "Epoch 36/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 23.4132 - mse: 23.4132 - mae: 3.6858 - val_loss: 21.1734 - val_mse: 21.1734 - val_mae: 3.5720\n",
            "Epoch 37/120\n",
            "240/240 [==============================] - 0s 988us/step - loss: 22.7460 - mse: 22.7460 - mae: 3.6783 - val_loss: 20.6448 - val_mse: 20.6448 - val_mae: 3.5477\n",
            "Epoch 38/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 22.2494 - mse: 22.2494 - mae: 3.6533 - val_loss: 20.3978 - val_mse: 20.3978 - val_mae: 3.5634\n",
            "Epoch 39/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.8831 - mse: 21.8831 - mae: 3.6425 - val_loss: 20.1027 - val_mse: 20.1027 - val_mae: 3.5632\n",
            "Epoch 40/120\n",
            "240/240 [==============================] - 0s 990us/step - loss: 21.6414 - mse: 21.6414 - mae: 3.6382 - val_loss: 19.9349 - val_mse: 19.9349 - val_mae: 3.5611\n",
            "Epoch 41/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.4682 - mse: 21.4682 - mae: 3.6425 - val_loss: 19.7352 - val_mse: 19.7352 - val_mae: 3.5530\n",
            "Epoch 42/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.3625 - mse: 21.3625 - mae: 3.6365 - val_loss: 19.5969 - val_mse: 19.5969 - val_mae: 3.5306\n",
            "Epoch 43/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.3107 - mse: 21.3107 - mae: 3.6432 - val_loss: 19.8888 - val_mse: 19.8888 - val_mae: 3.5912\n",
            "Epoch 44/120\n",
            "240/240 [==============================] - 0s 994us/step - loss: 21.1900 - mse: 21.1900 - mae: 3.6337 - val_loss: 19.7539 - val_mse: 19.7539 - val_mae: 3.5830\n",
            "Epoch 45/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.2020 - mse: 21.2020 - mae: 3.6382 - val_loss: 19.5621 - val_mse: 19.5621 - val_mae: 3.5587\n",
            "Epoch 46/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1730 - mse: 21.1730 - mae: 3.6363 - val_loss: 19.4440 - val_mse: 19.4440 - val_mae: 3.5326\n",
            "Epoch 47/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1711 - mse: 21.1711 - mae: 3.6361 - val_loss: 19.6209 - val_mse: 19.6209 - val_mae: 3.5356\n",
            "Epoch 48/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1722 - mse: 21.1722 - mae: 3.6353 - val_loss: 19.5307 - val_mse: 19.5307 - val_mae: 3.5504\n",
            "Epoch 49/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1500 - mse: 21.1500 - mae: 3.6319 - val_loss: 19.4502 - val_mse: 19.4502 - val_mae: 3.5441\n",
            "Epoch 50/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1211 - mse: 21.1211 - mae: 3.6328 - val_loss: 19.8722 - val_mse: 19.8722 - val_mae: 3.5830\n",
            "Epoch 51/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1070 - mse: 21.1070 - mae: 3.6322 - val_loss: 19.7058 - val_mse: 19.7058 - val_mae: 3.5883\n",
            "Epoch 52/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1387 - mse: 21.1387 - mae: 3.6481 - val_loss: 19.6828 - val_mse: 19.6828 - val_mae: 3.5330\n",
            "Epoch 53/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1541 - mse: 21.1541 - mae: 3.6395 - val_loss: 19.4117 - val_mse: 19.4117 - val_mae: 3.5464\n",
            "Epoch 54/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1775 - mse: 21.1775 - mae: 3.6404 - val_loss: 19.7311 - val_mse: 19.7311 - val_mae: 3.5808\n",
            "Epoch 55/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0857 - mse: 21.0857 - mae: 3.6374 - val_loss: 19.4999 - val_mse: 19.4999 - val_mae: 3.5505\n",
            "Epoch 56/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1643 - mse: 21.1643 - mae: 3.6423 - val_loss: 19.8276 - val_mse: 19.8276 - val_mae: 3.5841\n",
            "Epoch 57/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1969 - mse: 21.1969 - mae: 3.6416 - val_loss: 19.7699 - val_mse: 19.7699 - val_mae: 3.5887\n",
            "Epoch 58/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1005 - mse: 21.1005 - mae: 3.6323 - val_loss: 19.9532 - val_mse: 19.9532 - val_mae: 3.5457\n",
            "Epoch 59/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1691 - mse: 21.1691 - mae: 3.6508 - val_loss: 19.4293 - val_mse: 19.4293 - val_mae: 3.5452\n",
            "Epoch 60/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.2201 - mse: 21.2201 - mae: 3.6483 - val_loss: 19.5439 - val_mse: 19.5439 - val_mae: 3.5561\n",
            "Epoch 61/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.2544 - mse: 21.2544 - mae: 3.6466 - val_loss: 19.6168 - val_mse: 19.6168 - val_mae: 3.5757\n",
            "Epoch 62/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1030 - mse: 21.1030 - mae: 3.6405 - val_loss: 19.6808 - val_mse: 19.6808 - val_mae: 3.5361\n",
            "Epoch 63/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1746 - mse: 21.1746 - mae: 3.6462 - val_loss: 19.4049 - val_mse: 19.4049 - val_mae: 3.5459\n",
            "Epoch 64/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1233 - mse: 21.1233 - mae: 3.6367 - val_loss: 20.0836 - val_mse: 20.0836 - val_mae: 3.5698\n",
            "Epoch 65/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1414 - mse: 21.1414 - mae: 3.6462 - val_loss: 19.9201 - val_mse: 19.9201 - val_mae: 3.5697\n",
            "Epoch 66/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0970 - mse: 21.0970 - mae: 3.6430 - val_loss: 19.4663 - val_mse: 19.4663 - val_mae: 3.5347\n",
            "Epoch 67/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1473 - mse: 21.1473 - mae: 3.6438 - val_loss: 19.4879 - val_mse: 19.4879 - val_mae: 3.5587\n",
            "Epoch 68/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1894 - mse: 21.1894 - mae: 3.6491 - val_loss: 19.6539 - val_mse: 19.6539 - val_mae: 3.5826\n",
            "Epoch 69/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1131 - mse: 21.1131 - mae: 3.6320 - val_loss: 19.4879 - val_mse: 19.4879 - val_mae: 3.5332\n",
            "Epoch 70/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1748 - mse: 21.1748 - mae: 3.6479 - val_loss: 19.6110 - val_mse: 19.6110 - val_mae: 3.5613\n",
            "Epoch 71/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0646 - mse: 21.0646 - mae: 3.6345 - val_loss: 19.5252 - val_mse: 19.5252 - val_mae: 3.5588\n",
            "Epoch 72/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1557 - mse: 21.1557 - mae: 3.6471 - val_loss: 19.4659 - val_mse: 19.4659 - val_mae: 3.5445\n",
            "Epoch 73/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0697 - mse: 21.0697 - mae: 3.6338 - val_loss: 19.4810 - val_mse: 19.4810 - val_mae: 3.5535\n",
            "Epoch 74/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1038 - mse: 21.1038 - mae: 3.6333 - val_loss: 19.7228 - val_mse: 19.7228 - val_mae: 3.5638\n",
            "Epoch 75/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.2135 - mse: 21.2135 - mae: 3.6518 - val_loss: 19.4555 - val_mse: 19.4555 - val_mae: 3.5539\n",
            "Epoch 76/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.2352 - mse: 21.2352 - mae: 3.6457 - val_loss: 19.4841 - val_mse: 19.4841 - val_mae: 3.5334\n",
            "Epoch 77/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0286 - mse: 21.0286 - mae: 3.6303 - val_loss: 19.7569 - val_mse: 19.7569 - val_mae: 3.5907\n",
            "Epoch 78/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1264 - mse: 21.1264 - mae: 3.6451 - val_loss: 19.4385 - val_mse: 19.4385 - val_mae: 3.5517\n",
            "Epoch 79/120\n",
            "240/240 [==============================] - 0s 999us/step - loss: 21.1115 - mse: 21.1115 - mae: 3.6378 - val_loss: 20.0859 - val_mse: 20.0859 - val_mae: 3.5584\n",
            "Epoch 80/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1168 - mse: 21.1168 - mae: 3.6369 - val_loss: 19.5849 - val_mse: 19.5849 - val_mae: 3.5574\n",
            "Epoch 81/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1557 - mse: 21.1557 - mae: 3.6406 - val_loss: 19.4477 - val_mse: 19.4477 - val_mae: 3.5491\n",
            "Epoch 82/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1327 - mse: 21.1327 - mae: 3.6317 - val_loss: 19.4738 - val_mse: 19.4738 - val_mae: 3.5315\n",
            "Epoch 83/120\n",
            "240/240 [==============================] - 0s 998us/step - loss: 21.1267 - mse: 21.1267 - mae: 3.6415 - val_loss: 19.4045 - val_mse: 19.4045 - val_mae: 3.5364\n",
            "Epoch 84/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1195 - mse: 21.1195 - mae: 3.6384 - val_loss: 19.6787 - val_mse: 19.6787 - val_mae: 3.5352\n",
            "Epoch 85/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0601 - mse: 21.0601 - mae: 3.6370 - val_loss: 19.4736 - val_mse: 19.4736 - val_mae: 3.5581\n",
            "Epoch 86/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0585 - mse: 21.0585 - mae: 3.6352 - val_loss: 19.4529 - val_mse: 19.4529 - val_mae: 3.5355\n",
            "Epoch 87/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1233 - mse: 21.1233 - mae: 3.6487 - val_loss: 19.4085 - val_mse: 19.4085 - val_mae: 3.5414\n",
            "Epoch 88/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.2091 - mse: 21.2091 - mae: 3.6437 - val_loss: 20.1539 - val_mse: 20.1539 - val_mae: 3.6064\n",
            "Epoch 89/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1179 - mse: 21.1179 - mae: 3.6401 - val_loss: 19.8572 - val_mse: 19.8572 - val_mae: 3.5801\n",
            "Epoch 90/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1641 - mse: 21.1641 - mae: 3.6411 - val_loss: 19.7238 - val_mse: 19.7238 - val_mae: 3.5784\n",
            "Epoch 91/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1257 - mse: 21.1257 - mae: 3.6405 - val_loss: 19.4267 - val_mse: 19.4267 - val_mae: 3.5473\n",
            "Epoch 92/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1170 - mse: 21.1170 - mae: 3.6370 - val_loss: 19.6096 - val_mse: 19.6096 - val_mae: 3.5572\n",
            "Epoch 93/120\n",
            "240/240 [==============================] - 0s 996us/step - loss: 21.0529 - mse: 21.0529 - mae: 3.6318 - val_loss: 20.1367 - val_mse: 20.1367 - val_mae: 3.6064\n",
            "Epoch 94/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.3080 - mse: 21.3080 - mae: 3.6619 - val_loss: 19.6327 - val_mse: 19.6327 - val_mae: 3.5434\n",
            "Epoch 95/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1071 - mse: 21.1071 - mae: 3.6338 - val_loss: 19.5774 - val_mse: 19.5774 - val_mae: 3.5677\n",
            "Epoch 96/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1503 - mse: 21.1503 - mae: 3.6424 - val_loss: 19.5216 - val_mse: 19.5216 - val_mae: 3.5623\n",
            "Epoch 97/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1220 - mse: 21.1220 - mae: 3.6442 - val_loss: 19.4223 - val_mse: 19.4223 - val_mae: 3.5494\n",
            "Epoch 98/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0936 - mse: 21.0936 - mae: 3.6355 - val_loss: 19.5286 - val_mse: 19.5286 - val_mae: 3.5681\n",
            "Epoch 99/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0992 - mse: 21.0992 - mae: 3.6404 - val_loss: 19.8820 - val_mse: 19.8820 - val_mae: 3.6095\n",
            "Epoch 100/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1041 - mse: 21.1041 - mae: 3.6404 - val_loss: 20.4285 - val_mse: 20.4285 - val_mae: 3.6287\n",
            "Epoch 101/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1635 - mse: 21.1635 - mae: 3.6418 - val_loss: 19.3702 - val_mse: 19.3702 - val_mae: 3.5400\n",
            "Epoch 102/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0709 - mse: 21.0709 - mae: 3.6364 - val_loss: 19.4977 - val_mse: 19.4977 - val_mae: 3.5543\n",
            "Epoch 103/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1024 - mse: 21.1024 - mae: 3.6405 - val_loss: 19.7414 - val_mse: 19.7414 - val_mae: 3.5908\n",
            "Epoch 104/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0863 - mse: 21.0863 - mae: 3.6342 - val_loss: 19.8297 - val_mse: 19.8297 - val_mae: 3.5361\n",
            "Epoch 105/120\n",
            "240/240 [==============================] - 0s 986us/step - loss: 21.1108 - mse: 21.1108 - mae: 3.6394 - val_loss: 19.4044 - val_mse: 19.4044 - val_mae: 3.5334\n",
            "Epoch 106/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0560 - mse: 21.0560 - mae: 3.6289 - val_loss: 19.6040 - val_mse: 19.6040 - val_mae: 3.5571\n",
            "Epoch 107/120\n",
            "240/240 [==============================] - 0s 990us/step - loss: 21.1118 - mse: 21.1118 - mae: 3.6401 - val_loss: 19.6100 - val_mse: 19.6100 - val_mae: 3.5777\n",
            "Epoch 108/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1266 - mse: 21.1266 - mae: 3.6396 - val_loss: 19.3470 - val_mse: 19.3470 - val_mae: 3.5374\n",
            "Epoch 109/120\n",
            "240/240 [==============================] - 0s 989us/step - loss: 21.1447 - mse: 21.1447 - mae: 3.6388 - val_loss: 19.8991 - val_mse: 19.8991 - val_mae: 3.5868\n",
            "Epoch 110/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.2465 - mse: 21.2465 - mae: 3.6528 - val_loss: 19.4250 - val_mse: 19.4250 - val_mae: 3.5528\n",
            "Epoch 111/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1181 - mse: 21.1181 - mae: 3.6390 - val_loss: 19.8833 - val_mse: 19.8833 - val_mae: 3.5370\n",
            "Epoch 112/120\n",
            "240/240 [==============================] - 0s 992us/step - loss: 21.1676 - mse: 21.1676 - mae: 3.6420 - val_loss: 19.5324 - val_mse: 19.5324 - val_mae: 3.5371\n",
            "Epoch 113/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1746 - mse: 21.1746 - mae: 3.6424 - val_loss: 19.5943 - val_mse: 19.5943 - val_mae: 3.5389\n",
            "Epoch 114/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1723 - mse: 21.1723 - mae: 3.6401 - val_loss: 19.8326 - val_mse: 19.8326 - val_mae: 3.5542\n",
            "Epoch 115/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1307 - mse: 21.1307 - mae: 3.6407 - val_loss: 19.4076 - val_mse: 19.4076 - val_mae: 3.5302\n",
            "Epoch 116/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.0737 - mse: 21.0737 - mae: 3.6354 - val_loss: 19.5151 - val_mse: 19.5151 - val_mae: 3.5511\n",
            "Epoch 117/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.2032 - mse: 21.2032 - mae: 3.6509 - val_loss: 19.9316 - val_mse: 19.9316 - val_mae: 3.5619\n",
            "Epoch 118/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.2340 - mse: 21.2340 - mae: 3.6474 - val_loss: 19.3876 - val_mse: 19.3876 - val_mae: 3.5462\n",
            "Epoch 119/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1002 - mse: 21.1002 - mae: 3.6397 - val_loss: 20.0243 - val_mse: 20.0243 - val_mae: 3.5766\n",
            "Epoch 120/120\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 21.1644 - mse: 21.1644 - mae: 3.6456 - val_loss: 19.5822 - val_mse: 19.5822 - val_mae: 3.5256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f41185e0c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEpuzO7EUk1e",
        "outputId": "1226e2ac-e287-463c-b995-02cbaa60e6b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "ann.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_31 (Dense)             (None, 3)                 15        \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 31\n",
            "Trainable params: 31\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJj5k2MxZga3"
      },
      "source": [
        "## Calculating MSE, MAE and making the predictions for Sheet 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIyEeQdRZwgs",
        "outputId": "dae3183d-92e6-4a8f-c530-a7ec6e542c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "y_pred = ann.predict(X_test)\n",
        "print(y_pred)\n",
        "print(y_test)\n",
        "#print(np.concatenate((y_pred, y_test.reshape(len(y_test),1)),1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[430.54395]\n",
            " [458.36487]\n",
            " [462.87994]\n",
            " ...\n",
            " [469.53876]\n",
            " [441.64746]\n",
            " [461.63715]]\n",
            "[431.23 460.01 461.14 ... 473.26 438.   463.28]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMKmlzQkftCO",
        "outputId": "b710e1d5-816f-4073-c7b7-42ac58a8dfcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#MSE and MAE#\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "print(mean_squared_error(y_test, y_pred))#test mse\n",
        "print(mean_absolute_error(y_test,y_pred))#test mae"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19.582243048915974\n",
            "3.5256163708917025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aCnF7sDF1L7"
      },
      "source": [
        "*For the final model*,\n",
        "\n",
        "**Training** set: **mse**: 21.1644 , **mae**: 3.6456\n",
        "\n",
        "**Testing** set: **mse**: 19.5822, **mae**: 3.5256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_XPSEdC5KOp"
      },
      "source": [
        "##Now make prediction for Sheet 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoNPg3ZyiZc0",
        "outputId": "a1df9f43-e2a4-4dc8-ad85-729f47356b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Run on SHEET 2#\n",
        "sheet2= pd.read_excel('Folds5x2_pp.xlsx',sheet_name=1)\n",
        "print(sheet2)\n",
        "#Separate into input X,Y vars fromm sheet2\n",
        "X_sheet=sheet2.iloc[:,0:-1].values\n",
        "Y_sheet=sheet2.iloc[:,-1].values\n",
        "\n",
        "pred=sc.transform(X_sheet)#apply feature scaling on all input variables in Sheet 2\n",
        "sheet2_y_pred=ann.predict(pred) #predict y in sheet 2\n",
        "\n",
        "\n",
        "print(sheet2_y_pred)\n",
        "print(Y_sheet)\n",
        "\n",
        "#Concatenate prediction with actual values#\n",
        "print(np.concatenate((sheet2_y_pred, Y_sheet.reshape(len(Y_sheet),1)),1))\n",
        "\n",
        "#MSE for the sheet 2 predicted vs actual\n",
        "print(mean_squared_error(sheet2_y_pred, Y_sheet)) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         AT      V       AP     RH      PE\n",
            "0      9.59  38.56  1017.01  60.10  481.30\n",
            "1     12.04  42.34  1019.72  94.67  465.36\n",
            "2     13.87  45.08  1024.42  81.69  465.48\n",
            "3     13.72  54.30  1017.89  79.08  467.05\n",
            "4     15.14  49.64  1023.78  75.00  463.58\n",
            "...     ...    ...      ...    ...     ...\n",
            "9563  17.10  49.69  1005.53  81.82  457.32\n",
            "9564  24.73  65.34  1015.42  52.80  446.92\n",
            "9565  30.44  56.24  1005.19  56.24  429.34\n",
            "9566  23.00  66.05  1020.61  80.29  421.57\n",
            "9567  17.75  49.25  1020.86  63.67  454.41\n",
            "\n",
            "[9568 rows x 5 columns]\n",
            "[[480.0119 ]\n",
            " [469.38785]\n",
            " [467.22083]\n",
            " ...\n",
            " [433.84286]\n",
            " [443.82776]\n",
            " [460.81982]]\n",
            "[481.3  465.36 465.48 ... 429.34 421.57 454.41]\n",
            "[[480.01190186 481.3       ]\n",
            " [469.3878479  465.36      ]\n",
            " [467.2208252  465.48      ]\n",
            " ...\n",
            " [433.84286499 429.34      ]\n",
            " [443.82775879 421.57      ]\n",
            " [460.81982422 454.41      ]]\n",
            "20.772329908675957\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}